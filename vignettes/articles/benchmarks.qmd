---
title: "Benchmarks"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

## Goals

First, I want to measure nanoparquet's speed relative to good quality
CSV readers and writers, and also look at the sizes of the Parquet and
CSV files.

Second, I want to see how nanoparquet fares relative to other Parquet
implementations available from R.

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  comment = "#>"
)
me <- normalizePath(
  if (Sys.getenv("QUARTO_DOCUMENT_PATH") != "") {
    Sys.getenv("QUARTO_DOCUMENT_PATH")
  } else if (file.exists("benchmarks-funcs.R")) {
    getwd()
  } else if (file.exists("articles/benchmarks-funcs.R")) {
    "articles"
  } else {
    "vignettes/articles"
  })
writeLines(me, "/tmp/me")
source(file.path(me, "benchmarks-funcs.R"))
```

```{r, include = FALSE}
library(dplyr)
library(gt)
library(gtExtras)
```

```r
library(dplyr)
library(gt)
library(gtExtras)
```

## Data sets

I used use three data sets: small, medium and large. The small data set is
the `nycflights13::flights` data set, as is. The medium data set contains
20 copies of the small data set. The large data set contains 200 copies
of the small data set. See the `gen_data()` function in the
[`benchmark-funcs.R` file](
  https://github.com/r-lib/nanoparquet/blob/main/vignettes/articles/benchmarks-funcs.R)
in the nanoparquet GitHub repository.

Some basic information about each data set:
```{r, info}
if (file.exists(file.path(me, "data-info.parquet"))) {
  info_tab <- nanoparquet::read_parquet(file.path(me, "data-info.parquet"))
} else {
  get_data_info <- function(x) {
    list(dim = dim(x), size = object.size(x))
  }
  info <- lapply(data_sizes, function(s) get_data_info(gen_data(s)))
  info_tab <- data.frame(
    check.names = FALSE,
    name = data_sizes,
    rows = sapply(info, "[[", "dim")[1,],
    columns = sapply(info, "[[", "dim")[2,],
    "size in memory" = sapply(info, "[[", "size")
  )
  nanoparquet::write_parquet(info_tab, file.path(me, "data-info.parquet"))
}
info_tab |>
  gt() |>
  tab_header(title = "Data sets") |>
  tab_options(table.align = "left") |>
  fmt_integer() |>
  fmt_bytes(columns = "size in memory")
```

A quick look at the data:
```{r}
head(nycflights13::flights)
dplyr::glimpse(nycflights13::flights)
```

## Parquet implementations

I ran nanoparquet, Arrow and DuckDB. I also ran data.table without and with
compression and readr, to read/write CSV files. I used the running time of
readr as the baseline.

I ran each benchmark three times and record the results of the third
run. This is to make sure that the data and the software is not swapped out
by the OS. (Except for readr on the large data set, because it would take
too long.)

```{r, benchmark}
if (file.exists(file.path(me, "results.parquet"))) {
  results <- nanoparquet::read_parquet(file.path(me, "results.parquet"))
} else {
  results <- NULL
  lapply(data_sizes[1:2], function(s) {
    lapply(variants, function(v) {
      r <- if (v == "readr" && s == "large") {
        measure(v, s)
      } else {
        measure(v, s)
        measure(v, s)
        measure(v, s)
      }
      results <<- rbind(results, r)
    })
  })
  nanoparquet::write_parquet(results, file.path(me, "results.parquet"))
}
```

I include the complete raw results at the end of this article.

## Parquet vs CSV

<!-- For most use cases the Parquet format is superior to CSV files:

- Parquet has a rich type system, including native support for missing
  values.
- Parquet is binary, (possibly) efficiently encoded, (usually) compressed,
  so Parquet files are typicallly smaller, and faster to read and write.
- Parquet better supports reading subsets of rows or columns from a file,
  even through HTTP if you like.

One (the only?) great advantage of CSV files is that they are line-oriented
text files, so you can view and manipulate them with a lot of tools.
(As long as they can operate on large files, if your files are large.)

Being a simple format, CSV is also easy and fast to write, even
concurrently. -->

The results, focusing on the CSV readers and nanoparquet:

```{r, parquet-vs-csv-read}
csv_tab_read <- results |>
  filter(software %in% c("nanoparquet", "data.table", "data.table.gz", "readr")) |>
  filter(direction == "read") |>
  mutate(software = case_when(
    software ==  "data.table.gz" ~ "data.table (compressed)",
    .default = software
  )) |>
  rename(`data size` = data_size, time = time_elapsed) |>
  mutate(memory = mem_max_after - mem_before) |>
  mutate(base = tail(time, 1), .by = `data size`) |>
  mutate(speedup = base / time, x = round(speedup, digits = 1)) |>
  select(`data size`, software, time, x, speedup, memory) |>
  mutate(rawtime = time, time = prettyunits::pretty_sec(time)) |>
  rename(`speedup from CSV` = speedup)

csv_tab_read |>
  gt() |>
  tab_header(title = "Parquet vs CSV, reading") |>
  tab_options(table.align = "left") |>
  tab_row_group(md("**small data**"), rows = `data size` == "small", "s") |>
  tab_row_group(md("**medium data**"), rows = `data size` == "medium", "m") |>
  tab_row_group(md("**large data**"), rows = `data size` == "large", "l") |>
  row_group_order(c("s", "m", "l")) |>
  cols_hide(columns = c(`data size`, rawtime)) |>
  cols_align(columns = time, align = "right") |>
  fmt_bytes(columns = memory) |>
  gt_plt_bar(column = `speedup from CSV`)
```

Notes:

* The single-threaded nanoparquet Parquet-reader is competitive. It can
  read a compressed Parquet file just as fast as the state of the art
  uncompressed CSV reader that uses at least 2 threads.

The nanoparquet vs CSV results when writing Parquet or CSV files:

```{r, parquet-vs-csv-write}
csv_tab_write <- results |>
  filter(software %in% c("nanoparquet", "data.table", "data.table.gz", "readr")) |>
  filter(direction == "write") |>
  mutate(software = case_when(
    software ==  "data.table.gz" ~ "data.table (compressed)",
    .default = software
  )) |>
  rename(`data size` = data_size, time = time_elapsed, `file size` = file_size) |>
  mutate(memory = mem_max_after - mem_before) |>
  mutate(base = tail(time, 1), .by = `data size`) |>
  mutate(speedup = base / time, x = round(speedup, digits = 1)) |>
  select(`data size`, software, time, x, speedup, memory, `file size`) |>
  mutate(rawtime = time, time = prettyunits::pretty_sec(time)) |>
  rename(`speedup from CSV` = speedup)

csv_tab_write |>
  gt() |>
  tab_header(title = "Parquet vs CSV, writing") |>
  tab_options(table.align = "left") |>
  tab_row_group(md("**small data**"), rows = `data size` == "small", "s") |>
  tab_row_group(md("**medium data**"), rows = `data size` == "medium", "m") |>
  tab_row_group(md("**large data**"), rows = `data size` == "large", "l") |>
  row_group_order(c("s", "m", "l")) |>
  cols_hide(columns = c(`data size`, rawtime)) |>
  cols_align(columns = time, align = "right") |>
  fmt_bytes(columns = c(memory, `file size`)) |>
  gt_plt_bar(column = `speedup from CSV`)
```

Notes:

* The data.table CSV writer is about 3 times as fast as the nanoparquet
  Parquet writer, if the CSV file is uncompressed. The CSV writer uses at
  least 4 threads, the Parquet write is single-threaded.
* The nanoparquet Parquet writer is 2-5 times faster than the data.table
  CSV writer if the CSV file is compressed.
* The Parquet files are about 5-6 times smaller than the uncompressed CSV
  files and about 30-35% smaller than the compressed CSV files.

## Parquet implementations

This is the summary of the Parquet readers, for the same three files.

```{r, parquet-read}
pq_tab_read <- results |>
  filter(software %in% c("nanoparquet", "arrow", "duckdb", "readr")) |>
  filter(direction == "read") |>
  rename(`data size` = data_size, time = time_elapsed) |>
  mutate(memory = mem_max_after - mem_before) |>
  mutate(base = tail(time, 1), .by = `data size`) |>
  mutate(speedup = base / time, x = round(speedup, digits = 1)) |>
  select(`data size`, software, time, x, speedup, memory) |>
  mutate(rawtime = time, time = prettyunits::pretty_sec(time)) |>
  filter(software %in% c("nanoparquet", "arrow", "duckdb")) |>
  mutate(software = case_when(
    software ==  "arrow" ~ "Arrow",
    software == "duckdb" ~ "DuckDB",
    .default = software
  )) |>
  rename(`speedup from CSV` = speedup)

pq_tab_read |>
  gt() |>
  tab_header(title = "Parquet implementations, reading") |>
  tab_options(table.align = "left") |>
  tab_row_group(md("**small data**"), rows = `data size` == "small", "s") |>
  tab_row_group(md("**medium data**"), rows = `data size` == "medium", "m") |>
  tab_row_group(md("**large data**"), rows = `data size` == "large", "l") |>
  row_group_order(c("s", "m", "l")) |>
  cols_hide(columns = c(`data size`, rawtime)) |>
  cols_align(columns = time, align = "right") |>
  fmt_bytes(columns = memory) |>
  gt_plt_bar(column = `speedup from CSV`)
```

Notes:

* In general, all three implementations perform similarly. nanoparquet is
  very competitive for these three data sets in terms of speed and also
  tends to use the least amount of memory.
* I turned off ALTREP in arrow, so that it reads the data into memory.

The summary for the Parquet writers:

```{r, parquet-write}
pq_tab_write <- results |>
  filter(software %in% c("nanoparquet", "arrow", "duckdb", "readr")) |>
  filter(direction == "write") |>
  rename(`data size` = data_size, time = time_elapsed, `file size` = file_size) |>
  mutate(memory = mem_max_after - mem_before) |>
  mutate(base = tail(time, 1), .by = `data size`) |>
  mutate(speedup = base / time, x = round(speedup, digits = 1)) |>
  select(`data size`, software, time, x, speedup, memory, `file size`) |>
  mutate(rawtime = time, time = prettyunits::pretty_sec(time)) |>
  filter(software %in% c("nanoparquet", "arrow", "duckdb", "readr")) |>
  mutate(software = case_when(
    software ==  "arrow" ~ "Arrow",
    software == "duckdb" ~ "DuckDB",
    .default = software
  )) |>
  rename(`speedup from CSV` = speedup)

pq_tab_write |>
  gt() |>
  tab_header(title = "Parquet implementations, writing") |>
  tab_options(table.align = "left") |>
  tab_row_group(md("**small data**"), rows = `data size` == "small", "s") |>
  tab_row_group(md("**medium data**"), rows = `data size` == "medium", "m") |>
  tab_row_group(md("**large data**"), rows = `data size` == "large", "l") |>
  row_group_order(c("s", "m", "l")) |>
  cols_hide(columns = c(`data size`, rawtime)) |>
  cols_align(columns = time, align = "right") |>
  fmt_bytes(columns = c(memory, `file size`)) |>
  gt_plt_bar(column = `speedup from CSV`)
```

Notes:

* nanoparquet is again very competitive in terms of speed, it is slightly
  faster than the other two implementations, for these data sets.
* DuckDB seems to waste space when writing out Parquet files. This
  could be possibly fine tuned by forcing a different encoding. This
  behavior will improve with the forthcoming DuckDB 1.2.0 release, see also
  <https://github.com/duckdb/duckdb/issues/3316>.

<!-- ## ALTREP vs subsets -->

## Conclusions

These results will probably change for a different data sets, or on a
different system. In particular, Arrow and DuckDB are probably faster on
larger systems, where the data is stored on multiple physical disks.

Both Arrow and DuckDB let you run queries on the data without loading it
all into memory first. This is especially important if the data does not
fit into memory at all, not even the columns needed for the analysis.
nanoparquet cannot do this.

However, in general, based on these benchmarks I have good reasons to
trust that the nanoparquet Parquet reader and writer is competitive with
the other implementations available from R, both in terms of speed and
memory use.

If the limitations of nanoparquet are not prohibitive for your
use case, it is a good choice for Parquet I/O.

## Raw benchmark results

These are the raw results. You can scroll to the right if your screen is
not wide enough for the whole table.

```{r results}
#| R.options = list(width = 500)
print(results, n = Inf)
```

Notes:

* User time (`time_user`) plus system time (`time_system`) can be larger
  than the elapsed time (`time_elapsed`) for multithreaded
  implementations and it indeed is for all tools, except for nanoparquet,
  which is single-threaded.
* All memory columns are in bytes. `mem_before` is the RSS size before
  reading/writing. `mem_max_before` is the maximum RSS size of the process
  until then. `mem_max_after` is the maximum RSS size of the process
  _after_ the read/write operation.
* So I can calculate (estimate) the memory usage of the tool by
  subtracting `mem_before` from `mem_max_after`. This could overestimate
  the memory usage if `mem_max_after` were the same as `mem_max_before`,
  but this never happens in practice.
* When reading the file, `mem_max_after` includes the memory needed to
  store the data set itself. (See data sizes above.)
* For arrow, I turned off ALTREP using `options(arrow.use_altrep = FALSE)`,
  see the `benchmarks-funcs.R` file. Otherwise arrow does not actually
  read all the data into memory.

## About

See the [`benchmark-funcs.R`](
  https://github.com/r-lib/nanoparquet/blob/main/vignettes/articles/benchmarks-funcs.R)
file in the nanoparquet repository for the code of the benchmarks.

I ran each measurement in its own subprocess, to make it easier to measure
memory usage.

I did _not_ include the package loading time in the benchmarks.
nanoparquet has no dependencies and loads very quickly. Both the arrow and
duckdb packages might take up to 200ms to load on the test system,
because they need to load their dependencies and they are also bigger.

```{r session-info}
sessioninfo::session_info()
```
